{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fcbe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models, Input, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import uuid\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45321bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "lip_marks = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291, 78, 191, 80, 81, 82, 13, 312,\n",
    "             311, 310, 415, 308, 95, 88, 178, 87, 14, 317, 402, 318, 324, 146, 91, 181, 84,\n",
    "             17, 314, 405, 321, 375]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb8eb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "\n",
    "def extract_coordinates(results):\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]) if results.face_landmarks else np.zeros((468, 3))\n",
    "    pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]) if results.pose_landmarks else np.zeros((33, 3))\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]) if results.left_hand_landmarks else np.zeros((21, 3))\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]) if results.right_hand_landmarks else np.zeros((21, 3))\n",
    "    return np.concatenate([face, lh, pose, rh])\n",
    "\n",
    "def draw(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION,\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,255), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=1, circle_radius=0))\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(0,150,0), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(200,56,12), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(250,56,12), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(0,0,0), thickness=2, circle_radius=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c678b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function\n",
    "\n",
    "def load_json_file(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        sign_map = json.load(f)\n",
    "    return sign_map\n",
    "\n",
    "class CFG:\n",
    "    data_dir = \"asl-signs/\"\n",
    "    sequence_length = 12\n",
    "    rows_per_frame = 543\n",
    "\n",
    "ROWS_PER_FRAME = 543\n",
    "sequence = []\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    print('pq_path len: ', len(pq_path))\n",
    "    sequence.append(pq_path)\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "    \n",
    "sign_map = load_json_file(CFG.data_dir + 'sign_to_prediction_index_map.json')\n",
    "train_data = pd.read_csv(CFG.data_dir + 'train.csv')\n",
    "\n",
    "s2p_map = {k.lower():v for k,v in load_json_file(CFG.data_dir + \"sign_to_prediction_index_map.json\").items()}\n",
    "p2s_map = {v:k for k,v in load_json_file(CFG.data_dir + \"sign_to_prediction_index_map.json\").items()}\n",
    "encoder = lambda x: s2p_map.get(x.lower())\n",
    "decoder = lambda x: p2s_map.get(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547b5cfb",
   "metadata": {},
   "source": [
    "# Processing the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70bfc7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_asl(mode=0, video_path='', model=''):\n",
    "    \n",
    "    if model=='ann':\n",
    "        path='models/ann/model.tflite'\n",
    "    elif model=='top-01':\n",
    "        path='models/1st-place-solution-by-hoyso48/model.tflite'\n",
    "    elif model=='cnn':\n",
    "        path='models/cnn/model.tflite'\n",
    "    elif model=='cnn+3trans':\n",
    "        path='models/cnn+3trans/model.tflite'\n",
    "    elif model=='lstm':\n",
    "        path='models/lstm/model.tflite'\n",
    "    elif model=='transformer':\n",
    "        path='models/transformer/model.tflite'\n",
    "    else:\n",
    "        path='models/distance-angle-based-features-using-keras/model.tflite'\n",
    "        \n",
    "    interpreter = tf.lite.Interpreter(model_path=path)\n",
    "    interpreter.allocate_tensors()\n",
    "    print('Initilized Tensors')\n",
    "    found_signatures = list(interpreter.get_signature_list().keys())\n",
    "    prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "    \n",
    "    coordinates = []\n",
    "    res = []\n",
    "    \n",
    "    cap = cv2.VideoCapture(0) if mode==0 else cv2.VideoCapture(video_path)\n",
    "        \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        while not cap.isOpened():\n",
    "            print('Capture is Not Open')\n",
    "            cap = cv2.VideoCapture(0) if mode==0 else cv2.VideoCapture(video_path)\n",
    "            cv2.waitKey(1000)\n",
    "\n",
    "        pos_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "        print('Starting Prediction')\n",
    "        while True:\n",
    "            flag, image = cap.read()\n",
    "            if flag:\n",
    "                image.flags.writeable = False\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                results = holistic.process(image)\n",
    "                image.flags.writeable = True\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "                landmarks = extract_coordinates(results)\n",
    "#                 draw(image, results)\n",
    "                coordinates.append(landmarks)\n",
    "                if len(coordinates) == 15:\n",
    "                    prediction = prediction_fn(inputs=np.array(coordinates).astype(np.float32))\n",
    "                    sign = np.argmax(prediction[\"outputs\"])\n",
    "#                     print(f'{decoder(sign)} {format(prediction[\"outputs\"][sign]*10,\".2f\")}%')\n",
    "                    if model=='cnn+3trans' or model=='lstm':\n",
    "                        if prediction[\"outputs\"][0][sign]*10 > 50:\n",
    "                            res.append(decoder(sign))\n",
    "                    elif prediction[\"outputs\"][sign]*10 > 50:\n",
    "                            res.append(decoder(sign))\n",
    "                    cv2.putText(image, f\"Prediction:    {decoder(sign)}\", (3, 30),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                    coordinates = coordinates[10:]\n",
    "                    \n",
    "                \n",
    "                cv2.imshow('Prediction',image)\n",
    "\n",
    "            else:\n",
    "                # The next frame is not ready, so we try to read it again\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, pos_frame-1)\n",
    "                # It is better to wait for a while for the next frame to be ready\n",
    "                cv2.waitKey(1000)\n",
    "                break\n",
    "\n",
    "            if cv2.waitKey(10) & 0xFF == 27:\n",
    "                break\n",
    "            if cap.get(cv2.CAP_PROP_POS_FRAMES) == cap.get(cv2.CAP_PROP_FRAME_COUNT):\n",
    "                # If the number of captured frames is equal to the total number of frames,\n",
    "                # we stop\n",
    "                break\n",
    "    print(max(res, key=res.count) if len(res)>0 else \"Couldn't Predict\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40d3c581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initilized Tensors\n",
      "Starting Prediction\n",
      "apple\n"
     ]
    }
   ],
   "source": [
    "predict_asl(1, 'datasets/downloaded/animal/animal.mp4', model='cnn+3trans')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
